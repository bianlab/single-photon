{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70da4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from models.network_swinir import SwinIR as net\n",
    "from utils import util_calculate_psnr_ssim as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task=\"real_sr\"\n",
    "scale=4\n",
    "noise=15\n",
    "jpeg=40\n",
    "training_patch_size=128\n",
    "large_model=False\n",
    "model_path='./model_zoo/swinir/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth'\n",
    "folder_lq='./test/'\n",
    "tile=400\n",
    "tile_overlap=32\n",
    "save_dir='./results/'\n",
    "folder=folder_lq\n",
    "border = 0\n",
    "window_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3630b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f8075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(task,scale,large_model,model_path):\n",
    "    # 001 classical image sr\n",
    "    if task == 'classical_sr':\n",
    "        model = net(upscale=scale, in_chans=3, img_size=training_patch_size, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='pixelshuffle', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 002 lightweight image sr\n",
    "    # use 'pixelshuffledirect' to save parameters\n",
    "    elif task == 'lightweight_sr':\n",
    "        model = net(upscale=scale, in_chans=3, img_size=64, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='pixelshuffledirect', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 003 real-world image sr\n",
    "    elif task == 'real_sr':\n",
    "        if not large_model:\n",
    "            # use 'nearest+conv' to avoid block artifacts\n",
    "            model = net(upscale=4, in_chans=3, img_size=64, window_size=8,\n",
    "                        img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='1conv')\n",
    "        else:\n",
    "            # larger model size; use '3conv' to save parameters and memory; use ema for GAN training\n",
    "            model = net(upscale=4, in_chans=3, img_size=64, window_size=8,\n",
    "                        img_range=1., depths=[6, 6, 6, 6, 6, 6, 6, 6, 6], embed_dim=240,\n",
    "                        num_heads=[8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
    "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='3conv')\n",
    "        param_key_g = 'params_ema'\n",
    "\n",
    "    # 004 grayscale image denoising\n",
    "    elif task == 'gray_dn':\n",
    "        model = net(upscale=1, in_chans=1, img_size=128, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 005 color image denoising\n",
    "    elif task == 'color_dn':\n",
    "        model = net(upscale=1, in_chans=3, img_size=128, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 006 JPEG compression artifact reduction\n",
    "    # use window_size=7 because JPEG encoding uses 8x8; use img_range=255 because it's sligtly better than 1\n",
    "    elif task == 'jpeg_car':\n",
    "        model = net(upscale=1, in_chans=1, img_size=126, window_size=7,\n",
    "                    img_range=255., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    pretrained_model = torch.load(model_path)\n",
    "    model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cb1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img_lq,tile,tile_overlap, model, window_size, scale):\n",
    "    if tile is None:\n",
    "        # test the image as a whole\n",
    "        output = model(img_lq)\n",
    "    else:\n",
    "        # test the image tile by tile\n",
    "        b, c, h, w = img_lq.size()\n",
    "        tile = min(tile, h, w)\n",
    "        assert tile % window_size == 0, \"tile size should be a multiple of window_size\"\n",
    "        tile_overlap = tile_overlap\n",
    "        sf = scale\n",
    "\n",
    "        stride = tile - tile_overlap\n",
    "        h_idx_list = list(range(0, h-tile, stride)) + [h-tile]\n",
    "        w_idx_list = list(range(0, w-tile, stride)) + [w-tile]\n",
    "        E = torch.zeros(b, c, h*sf, w*sf).type_as(img_lq)\n",
    "        W = torch.zeros_like(E)\n",
    "\n",
    "        for h_idx in h_idx_list:\n",
    "            for w_idx in w_idx_list:\n",
    "                in_patch = img_lq[..., h_idx:h_idx+tile, w_idx:w_idx+tile]\n",
    "                out_patch = model(in_patch)\n",
    "                out_patch_mask = torch.ones_like(out_patch)\n",
    "\n",
    "                E[..., h_idx*sf:(h_idx+tile)*sf, w_idx*sf:(w_idx+tile)*sf].add_(out_patch)\n",
    "                W[..., h_idx*sf:(h_idx+tile)*sf, w_idx*sf:(w_idx+tile)*sf].add_(out_patch_mask)\n",
    "        output = E.div_(W)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcc8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(task,scale,large_model,model_path)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "./testsets/GT\\1.tiff\n",
      "1\n",
      "./testsets/GT\\10.tiff\n",
      "2\n",
      "./testsets/GT\\11.tiff\n",
      "3\n",
      "./testsets/GT\\12.tiff\n",
      "4\n",
      "./testsets/GT\\13.tiff\n",
      "5\n",
      "./testsets/GT\\14.tiff\n",
      "6\n",
      "./testsets/GT\\15.tiff\n",
      "7\n",
      "./testsets/GT\\16.tiff\n",
      "8\n",
      "./testsets/GT\\17.tiff\n",
      "9\n",
      "./testsets/GT\\18.tiff\n",
      "10\n",
      "./testsets/GT\\2.tiff\n",
      "11\n",
      "./testsets/GT\\3.tiff\n",
      "12\n",
      "./testsets/GT\\4.tiff\n",
      "13\n",
      "./testsets/GT\\5.tiff\n",
      "14\n",
      "./testsets/GT\\6.tiff\n",
      "15\n",
      "./testsets/GT\\7.tiff\n",
      "16\n",
      "./testsets/GT\\8.tiff\n",
      "17\n",
      "./testsets/GT\\9.tiff\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for idx, path in enumerate(sorted(glob.glob(os.path.join(folder, '*')))):\n",
    "    # read image\n",
    "    print(idx)\n",
    "    img_lq = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    print(path)\n",
    "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
    "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        # pad input image to be a multiple of window_size\n",
    "        _, _, h_old, w_old = img_lq.size()\n",
    "        h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "        w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "        output = test(img_lq,tile,tile_overlap, model, window_size, scale)\n",
    "        output = output[..., :h_old * scale, :w_old * scale]\n",
    "    # save image\n",
    "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "    if output.ndim == 3:\n",
    "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
    "    output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "    cv2.imwrite(f'{save_dir}/%d.jpg'%i, output)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f86d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f2e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6537d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13686d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef921ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd017e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06740d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8a60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2507a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ac7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3de71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cca74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce91e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d2057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e79db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0d1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176c0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fedae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f888986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2403bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30997470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d23a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3db54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb1f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec64270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c692a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb365d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fbd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716ecc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b426fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51003cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572fc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be343378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69c9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6aada8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13b74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742308e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ea156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02108bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081914c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07be57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df011057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
